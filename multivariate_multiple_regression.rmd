---
title: "Multivariate Multiple Regression"
output: html_document
---

```{r setup}
options(scipen = 999)
library(purrr)
library(dplyr)
library(data.table)
```


## Goal

Here we will perform a multivariate multiple regression analysis to predict a player's topline basketball in-game statistics using several variables such their salary, age, team, and certain other in-game statistics. This analysis is motivated by the question: what is the relationship between a player's salary and their in-game performance?

In this analysis, we investigate the inverse of the typical question: instead of asking how a player's in-game performance predicts their salary, we ask how a player's salary predicts their in-game performance. This can help us determine how related the salary is to performance, and may lead to insights in if a player is "overpaid" or "underpaid" relative to their performance.

We must use multivariate multiple regression because we have multiple response variables (e.g. points, assists, and rebounds) and multiple predictor variables (e.g. the salary, age, team, and other in-game statistics).

## Data

Read in the salary and player data.

```{r}
# list files in data/salary/src directory
dir <- "data/salary/src"
salary_files_tmp <- list.files(dir)
salary_files <- paste0(dir, "/", salary_files_tmp)

# read in salary data
salary_df <- map_df(salary_files, fread) %>% 
    as_tibble()

head(salary_df)
```

```{r}
# list files in data/gamelog/src directory
dir <- "data/gamelog/src/gamelogs/"
gamelog_files_tmp <- list.files(dir)
# remove files that start with a number
gamelog_files_tmp <- gamelog_files_tmp[!grepl("^[0-9]", gamelog_files_tmp)]
gamelog_files <- paste0(dir, "/", gamelog_files_tmp)

# read in gamelog data
# cast the GS column to character to avoid errors
gamelog_df <- map_df(gamelog_files, ~fread(.x) %>% mutate(across(.fns = as.character))) %>%
    as_tibble()

head(gamelog_df)
```

We merge the two data sets on player id.

```{r}
# merge the two data sets
df <- gamelog_df %>% 
    inner_join(salary_df, by = c("player" = "player_id"))

head(df)
```

Now we also get some player-level data.

```{r}
# list files in data/players/src directory
dir <- "data/players/src/"
players_files_tmp <- list.files(dir)
players_files <- paste0(dir, "/", players_files_tmp)

# read in players data
players_df <- map_df(players_files,  ~fread(.x) %>% mutate(across(.fns = as.character))) %>%
    as_tibble()

# we must create a player id column from the player_href column
# this is the string between /a/ and .html 
players_df <- players_df %>% 
    mutate(
        player_id = stringr::str_extract(player_href, "(?<=/a/).*(?=\\.html)")
    )

head(players_df)
```

We merge this to the main data set.

```{r}
# merge the two data sets
df2 <- df %>% 
    inner_join(players_df, by = c("player" = "player_id"))
```

Since the salary is a season-level statistic, we calculate season-level statistics for the game-level in-game statistics. 

```{r}
per_season_data_tmp <- df2 %>% 
    mutate(
        Age = substr(Age, 1, 2), # get first two characters of age to make numeric
        # convert salary to numeric, parsing out the $ and commas
        Salary = as.numeric(gsub("[$,]", "", Salary))
    ) %>%
    group_by(player,Season) %>%
    summarize(
        Age = mean(as.numeric(Age)),
        Tm = first(Tm),
        Team = first(Team),
        Pos = first(Pos),
        Ht = mean(as.numeric(Ht)),
        Wt = mean(as.numeric(Wt)),
        Colleges = first(Colleges),
        G = sum(as.numeric(G), na.rm = TRUE),
        GS = sum(as.numeric(GS), na.rm = TRUE),
        MP = mean(as.numeric(MP), na.rm = TRUE),
        FG = mean(as.numeric(FG), na.rm = TRUE),
        FGA = mean(as.numeric(FGA), na.rm = TRUE),
        `FG%` = mean(as.numeric(`FG%`), na.rm = TRUE),
        `3P` = mean(as.numeric(`3P`), na.rm = TRUE),
        `3PA` = mean(as.numeric(`3PA`), na.rm = TRUE),
        `3P%` = mean(as.numeric(`3P%`), na.rm = TRUE),
        FT = mean(as.numeric(FT), na.rm = TRUE),
        FTA = mean(as.numeric(FTA), na.rm = TRUE),
        `FT%` = mean(as.numeric(`FT%`), na.rm = TRUE),
        ORB = mean(as.numeric(ORB), na.rm = TRUE),
        DRB = mean(as.numeric(DRB), na.rm = TRUE),
        TRB = mean(as.numeric(TRB), na.rm = TRUE),
        AST = mean(as.numeric(AST), na.rm = TRUE),
        STL = mean(as.numeric(STL), na.rm = TRUE),
        BLK = mean(as.numeric(BLK), na.rm = TRUE),
        TOV = mean(as.numeric(TOV), na.rm = TRUE),
        PF = mean(as.numeric(PF), na.rm = TRUE),
        PTS = mean(as.numeric(PTS), na.rm = TRUE),
        Salary = mean(Salary, na.rm = TRUE)
    )

head(per_season_data_tmp)

# write tp csv
readr::write_csv(per_season_data_tmp, "data/per_season_data.csv")
```

## Exploratory Data Analysis

### Multivariate Normality

We recall from lecture that one of the assumptions of linear regression is that the data is multivariate normal. This distribution is characterized by the following properties:

1. The marginal distribution of each variable is normal
2. The joint distribution of pairs of variables are normal

These are very helpful because they allow us to check for multivariate normality by checking for univariate normality, rather than considering the challenging task of checking for multivariate normality directly.

We now consider the univeriate cases. We plot the distribution of each numeric column and the salary column in order to visually inspect the distribution. Then, we use the Shapiro-Wilk test to test for normality and check if this matches our visual inspection.

```{r}
# read the data from csv
per_season_data <- readr::read_csv("data/per_season_data.csv")
dim(per_season_data)
head(per_season_data)
```

```{r}
# handle NAs for now
per_season_data <- per_season_data %>% 
    mutate(
        Team = ifelse(is.na(Team), "Other", Team),
        Season = ifelse(is.na(Season), "Other", Season),
        Salary = ifelse(is.na(Salary), mean(Salary), Salary)
    )
```

```{r}
# create a list of numeric columns
num_cols <- per_season_data %>% 
    select(where(is.numeric)) %>% 
    names()

# plot the distribution of each numeric column
for (col in num_cols) {
    hist(per_season_data[[col]], main = col)
}

# plot the distribution of the salary
hist(per_season_data$Salary, main = "Salary")
```

We attempted to perform the Shapiro Wilk test, but it was not possible to perform the test on the size of our data - there is a limit of 5000 observations. So, we will perform the test on a sample of the data, but we will not use this test to make any final conclusions.

```{r}
# shapiro wilk test
for (col in num_cols) {
    print(col)
    # sample the data of size 5000
    x = sample(per_season_data[[col]], 5000)
    print(shapiro.test(x))
}
```

We see clearly that many of the variables are not normally distributed and will need to be transformed. Furthermore, the salary distribution features a long tail, with substantial outliers. 

## Transformations

Depending on the skewness of the data, we can consider polynomial transformations and log transformations. We will consider the following transformations:

```{r}
```

## Multivariate Multiple Regression Regression

Now that the data is more suitable for our regression task, we can perform the regression. First, we discuss the characteristics of multivariate multiple regression.

Most notably, multivariate multiple regression is a generalization of multiple regression. In multiple regression, we have a single response variable and multiple predictor variables. In multivariate multiple regression, we have multiple response variables and multiple predictor variables. 

To start, we create the following model:

$$Y = \Beta Z$$

```{r}
# convert some variabels to factor
per_season_data <- per_season_data %>% 
    mutate(
        Season = as.factor(Season),
        Team = as.factor(Team),
        Pos = as.factor(Pos)
    )

mlm1 <- lm(
    cbind(PTS, AST, TRB) ~ Season + Salary + Age + Ht + Wt + Pos + GS + G,
    data = per_season_data
)
summary(mlm1)
```

We see that this initial model is not very good. The R-squared is very low, and the p-values for the coefficients are very high. We will need to improve this model.

### Model Selection

We can use several methods to help us choose the best model. It is preferable to include the least number of predictor variables possible, in order to have an explanable model, avoid overfitting, etc, but to balance this with the need to have a model that is sufficiently accurate. 

For example, we may use the AIC, BIC, or adjusted R-squared to help us choose the best model. 

```{r}

```

```{r}
```


