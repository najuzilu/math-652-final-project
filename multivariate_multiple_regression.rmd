---
title: "Multivariate Multiple Regression"
output: html_document
---

```{r setup}
options(scipen = 999)
library(purrr)
library(dplyr)
library(data.table)
#library(glmnet)
library(tidyr)
set.seed(123)
```

## Goal

Here we will perform a multivariate multiple regression analysis to predict a player's topline basketball in-game statistics using several variables such their salary, age, team, and certain other in-game statistics. This analysis is motivated by the question: what is the relationship between a player's salary and their in-game performance?

In this analysis, we investigate the inverse of the typical question: instead of asking how a player's in-game performance predicts their salary, we ask how a player's salary predicts their in-game performance. This can help us determine how related the salary is to performance, and may lead to insights in if a player is "overpaid" or "underpaid" relative to their performance.

We must use multivariate multiple regression because we have multiple response variables (e.g. points, assists, and rebounds) and multiple predictor variables (e.g. the salary, age, team, and other in-game statistics).

## Data

See the data prep markdown for more details. 

## Exploratory Data Analysis

### Multivariate Normality

We recall from lecture that one of the assumptions of linear regression is that the data is multivariate normal. This distribution is characterized by the following properties:

1. The marginal distribution of each variable is normal
2. The joint distribution of pairs of variables are normal

These are very helpful because they allow us to check for multivariate normality by checking for univariate normality, rather than considering the challenging task of checking for multivariate normality directly.

We now consider the univeriate cases. We plot the distribution of each numeric column and the salary column in order to visually inspect the distribution. Then, we use the Shapiro-Wilk test to test for normality and check if this matches our visual inspection.

```{r}
# read the data from csv
per_season_data <- readr::read_csv("data/per_season_data_v2.csv")
dim(per_season_data)
head(per_season_data)
```

```{r}
# handle NAs for now
per_season_data <- per_season_data %>% 
    mutate(
        team = ifelse(is.na(team), "Other", team),
        Season = ifelse(is.na(Season), "Other", Season),
        salary = ifelse(is.na(salary), mean(salary), salary)
    )
```

```{r}
# create a list of numeric columns
num_cols <- per_season_data %>% 
    select(where(is.numeric)) %>% 
    names()

# plot the distribution of each numeric column
for (col in num_cols) {
    png(paste0("analyses/regression_results/plots/", col, "_histogram.png"))
    hist(per_season_data[[col]], main = col)
    dev.off()
}

# plot the distribution of the salary column
png("analyses/regression_results/plots/salary_histogram.png")
hist(per_season_data$salary, main = "Salary")
dev.off()
```

We also present Q-Q plots for the variables, which demonstrats the columns that aren't normal. 

```{r}
for (col in num_cols) {
    png(paste0("analyses/regression_results/plots/", col, "_qqplot.png"))
    qqnorm(per_season_data[[col]], main = col)
    qqline(per_season_data[[col]])
    # title
    title(main = col)
    dev.off()
}
```

We attempted to perform the Shapiro Wilk test, but it was not possible to perform the test on the size of our data - there is a limit of 5000 observations. So, we will perform the test on a sample of the data, but we will not use this test to make any final conclusions.

```{r, eval=FALSE}
# shapiro wilk test
for (col in num_cols) {
    print(col)
    # sample the data of size 5000
    x = sample(per_season_data[[col]], 5000)
    print(shapiro.test(x))
}
```

We see clearly that many of the variables are not normally distributed and will need to be transformed. Furthermore, the salary distribution features a long tail, with substantial outliers. 

## Transformations

Depending on the skewness of the data, we can consider polynomial transformations and log transformations. We will consider transformations for the following variables which will be used in the regression analysis:

* pts
* ast
* trb
* salary
* age
* ht
* wt
* gs
* g
* mp

After investigation, we see that the following transformations are suitable:

```{r}
per_season_data <- per_season_data %>% 
    mutate(
        pts = pts^(1/2),
        ast = ast^(1/2),
        trb = trb^(1/2),
        salary = salary^(1/10),
        #Age no transform needed
        #Ht no transform needed
        wt = wt^2,
        gs = gs^(1/3),
        g = g^(1/2)
        #MP no transform needed
    )
```

## Multivariate Multiple Regression Regression

Now that the data is more suitable for our regression task, we can perform the regression. First, we discuss the characteristics of multivariate multiple regression.

Most notably, multivariate multiple regression is a generalization of multiple regression. In multiple regression, we have a single response variable and multiple predictor variables. In multivariate multiple regression, we have multiple response variables and multiple predictor variables. 

To start, we create the following model:

$$Y = \Beta Z$$

```{r}
# convert some variabels to factor for R's lm function
per_season_data <- per_season_data %>% 
    mutate(
        Season = as.factor(Season),
        team = as.factor(team),
        pos = as.factor(pos)
    )
```

```{r}
mlm1 <- lm(
    cbind(pts, ast, trb) ~ salary + age + ht + wt + pos + gs + g + mp,
    data = per_season_data
)
summary(mlm1)

# save the summary to a file in regression_results/analyses
capture.output(summary(mlm1), file = "analyses/regression_results/mlm1.txt")
```

### Interpretation

```{r}
mlm1$coefficients

# save coefficients to csv
write.csv(mlm1$coefficients, "analyses/regression_results/mlm1_coefficients.csv")
```

We will now consider each predictor variable and see how it affects the response variables.

* `salary`: We see salary has a signicant effect at the 0.05 level only on Points and Rebounds, but does not have a significant effect on Assists. This is suprising, as it suggests that some in-game stats may not be as related to salary as others. It may suggest that the assist-seeking players may not be as highly compensated for that ability. The model confirms that the relatonship between salary and in-game statistical performance is positive. 
* `age`: This is a very significant variable and is negative for all three response variables. This suggests that as players become older, their statistical output goes down. This is not surprising.
* `ht`: This variable is not significant for points but it is for assists and rebounds. Height has a positive relationship with rebounds and a negative relationship with assists. This makes sense because certain basketball positions are determined by height and these positions lead to specific in-game roles which may lead to more assists or more rebounds, but all positions can score points. Taller players are more likely to be centers, who are more likely to get rebounds, and shorter players are more likely to be guards, who are more likely to get assists.
* `gs` and `g`: These variables are similar, so we will group them together. They are highly significant for the points response variable, but not so much for the assists and rebounds, which is a little surprising. Furthermore, the GS variable has a negative coefficient with points - highly surprising. This relationship may require further investigation.
* `mp`: This variable is significant and positive for all three response variables. This makes sense because players who play more minutes are more likely to score more points, get more assists, and get more rebounds.

### Model Fit

Before we proceed further, let us consider the fit of the model. We plot the residuals vs fitted values for each of the three response variables and check the normality of the residuals. 
    
```{r}
# plot each response residuals vs fitted values and save
png("analyses/regression_results/plots/mlm1_resid_fitted.png")
par(mfrow = c(3,1))
plot(mlm1$fit[,1],mlm1$resid[,1])
plot(mlm1$fit[,2],mlm1$resid[,2])
plot(mlm1$fit[,3],mlm1$resid[,3])
dev.off()
```

```{r}
# plot each resppnse residual qq plot and save
png("analyses/regression_results/plots/mlm1_resid_qq.png")
par(mfrow = c(3,1))
qqnorm(mlm1$resid[,1])
qqline(mlm1$resid[,1])
qqnorm(mlm1$resid[,2])
qqline(mlm1$resid[,2])
qqnorm(mlm1$resid[,3])
qqline(mlm1$resid[,3])
dev.off()
```

We see that this initial model is pretty good, but perhaps we can do better. We can consider alternate subsets of variables to include in the model. And compare them using the AIC metric. 

### Model Selection

We can use several methods to help us choose the best model. It is preferable to include the least number of predictor variables possible, in order to have an explanable model, avoid overfitting, etc, but to balance this with the need to have a model that is sufficiently accurate. 

For example, we may use the AIC, BIC, or adjusted R-squared to help us choose the best model. We calculate the AIC for our initial model. 
    
```{r}
# define AIC because the function doesnt support multiple respnses
# (resid sum of squares cross product matrix)/n
# AIC = n * log(det(Sigma_d)) - 2p * d
# where p is the number of parameters and d is the number of responses

n = nrow(per_season_data)
# mlm1 number of parameters
p = 8 
d = 3
aic = n * log(det(crossprod(mlm1$residuals))) - 2 * p * d
aic
```

We first consider the most simple model of only using `salary` variable to predict the in-game statistics. 

```{r}
salary_mlm <- lm(
    cbind(pts, ast, trb) ~ salary,
    data = per_season_data
)
summary(salary_mlm)
```

```{r}
# aic
n = nrow(per_season_data)
p = 1
d = 3
aic_salary = n * log(det(crossprod(salary_mlm$residuals))) - 2 * p * d
aic_salary
```

We see that the AIC value is much higher for the salary-only model, so we will not consider this model further. We will consider one more simpler model, which only includes the `salary` and demographic variables. 

```{r}
salary_demographic_mlm <- lm(
    cbind(pts, ast, trb) ~ salary + age + ht + wt,
    data = per_season_data
)

summary(salary_demographic_mlm)
```

```{r}
n = nrow(per_season_data)
p = 4
d = 3
aic_demo = n * log(det(crossprod(salary_demographic_mlm$residuals))) - 2 * p * d
aic_demo
```

This model also gives a much higher AIC than the initial model, so we will not consider this model further. Indeed, it appears that the initial model is a well performing model and that including all of those variables is beneficial. 

## Lasso Regression

We also consider using a lasso to do variable selection.

```{r}
# specify target and regressors

target <- per_season_data[,c("pts","ast","trb")]
rgrsrs <- per_season_data[,-c(1,9,24,25,30,31,32)]

# remove rows containing NAs
ind <- which(rowSums(is.na(rgrsrs)) > 0)
target <- target[-c(ind),]
rgrsrs <- rgrsrs[-c(ind),]

# run Lasso
lasso_mlm <- glmnet(rgrsrs, target, family = "mgaussian")

# look for a cutoff value for lambda
plot(lasso_mlm, xvar = "lambda", label = T, type.coef = "2norm")
print(lasso_mlm)
```
We see that at a value for lambda of around 0.25 the model has 7 non-zero coefficients and explains around 80 percent of the deviance in the data. This seems like a manageable number of features, and an acceptable amount of predictive power.

```{r}
# see which coefficients remain
coef(lasso_mlm, s =0.25)
```

The regressors selected by the lasso regression are ht, mp, fg, fga, orb, drb, and tov. Of these, ht and mp were both highly significant in the multivariate regression explored above.

Now we compute the AIC for the lasso regression. The code to do this was pulled from 
https://rdrr.io/github/3inar/einr/man/AIC.glmnet.html
```{r}
# compute AIC for the lasso regression
## Note that these values are suspiciously low, and this section needs work.
AIC.glmnet <- function(glm_fit) {
  chisqLR <- glm_fit$nulldev - deviance(glm_fit)

  chisqLR - 2*glm_fit$df
}

AIC.glmnet(lasso_mlm)
```
